{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Import the necessary packages\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nGbeLwng64uf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First build an abstract Layer class that will be used for linear and \n",
        "#activation layers\n",
        "#Both layers will contain forward and backward functions\n",
        "\n",
        "class Layer():\n",
        "    def __init__(self):\n",
        "        #Only shared variable is input\n",
        "        self.input = None\n",
        "        \n",
        "    def forward(self, input_data):\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def backward(self, error, lr):\n",
        "        raise NotImplementedError()"
      ],
      "metadata": {
        "id": "xo1AWH9T64rx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(Layer):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Linear, self).__init__()\n",
        "        self.weights = np.random.rand(input_size,output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "        \n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        out = np.dot(self.input, self.weights)+self.bias\n",
        "        return out\n",
        "    \n",
        "    def backward(self, error, lr):\n",
        "        input_error = np.dot(error, self.weights.T)\n",
        "        weight_error = np.dot(self.input.T, error)\n",
        "        \n",
        "        self.weights -= lr*weight_error\n",
        "        self.bias -= lr*error\n",
        "        \n",
        "        return input_error"
      ],
      "metadata": {
        "id": "fKKQfjUX64oi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "    def __init__(self, act, act_prime):\n",
        "        super(Activation, self).__init__()\n",
        "        self.activation = act\n",
        "        self.activation_prime = act_prime\n",
        "    \n",
        "    def forward(self, input_data):\n",
        "        self.input = input_data\n",
        "        return self.activation(self.input)\n",
        "    \n",
        "    def backward(self, error, lr):\n",
        "        error = self.activation_prime(self.input)*error\n",
        "        return error"
      ],
      "metadata": {
        "id": "_de43wyr64k5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Network():\n",
        "    def __init__(self, loss, loss_prime):\n",
        "        self.layers = []\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "        self.errors = []\n",
        "        self.epochs = None\n",
        "        \n",
        "    def add_layer(self, layer):\n",
        "        #Can add activation or linear layer here\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def predict(self, input_data):\n",
        "        input_size = len(input_data)\n",
        "        prediction = []\n",
        "        #Forward Propagation\n",
        "        for i in range(input_size):\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward(output)\n",
        "            prediction.append(output)\n",
        "        \n",
        "        return prediction\n",
        "    \n",
        "    def train(self, x, y, epochs, lr):\n",
        "        sample_size = len(x)\n",
        "        self.epochs = epochs\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            error = 0\n",
        "            for j in range(sample_size):\n",
        "                #Forward\n",
        "                output = x[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward(output)\n",
        "                \n",
        "                #Update error\n",
        "                error += self.loss(output, y[j])\n",
        "                running_error = self.loss_prime(output, y[j])\n",
        "                \n",
        "                #Backward\n",
        "                for layer in reversed(self.layers):\n",
        "                    running_error = layer.backward(running_error, lr)\n",
        "                    \n",
        "            error /= sample_size\n",
        "            self.errors.append(error)\n",
        "            print(\"Epoch %d/%d, Error = %f\" %(epoch+1, epochs, error))\n",
        "        \n",
        "    def plot_error(self, title):\n",
        "        #Method to plot errors after training\n",
        "        x = np.linspace(1, self.epochs, self.epochs)\n",
        "        y = self.errors\n",
        "        plt.plot(x, y)\n",
        "        plt.title(title)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Error')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "us8SF0Vf64Z8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Some activation functions\n",
        "\n",
        "#ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    x[x>0] = 1\n",
        "    x[x<=0] = 0\n",
        "    return x\n",
        "\n",
        "#Sigmoid\n",
        "def sigmoid(x):\n",
        "    e = 1/(1+np.exp(-x))\n",
        "    return e/e.sum(axis = 0)\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
        "\n",
        "#tanh\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-np.tanh(x)**2"
      ],
      "metadata": {
        "id": "h2SaFKp57ESm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Function\n",
        "def mse(y_pred, y):\n",
        "    return np.mean(np.power(y_pred - y, 2))\n",
        "\n",
        "def mse_prime(y_pred, y):\n",
        "    return 2*(y_pred - y)/float(y.size)"
      ],
      "metadata": {
        "id": "t8Sv7SY07HUI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# load MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# reshape and normalize input data, encode output data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# Network\n",
        "MyNet = Network(mse, mse_prime)\n",
        "MyNet.add_layer(Linear(28*28, 100))               \n",
        "MyNet.add_layer(Activation(tanh, tanh_prime))\n",
        "MyNet.add_layer(Linear(100, 100))                 \n",
        "MyNet.add_layer(Activation(tanh, tanh_prime))\n",
        "MyNet.add_layer(Linear(100, 50))                 \n",
        "MyNet.add_layer(Activation(tanh, tanh_prime))\n",
        "MyNet.add_layer(Linear(50, 10))                    \n",
        "MyNet.add_layer(Activation(tanh, tanh_prime))\n",
        "\n",
        "# train on 1000 samples\n",
        "MyNet.train(x_train[0:1000], y_train[0:1000], epochs=200, lr=0.09)\n",
        "\n",
        "# test on 3 samples\n",
        "out = MyNet.predict(x_test[0:3])\n",
        "print(\"\\n\")\n",
        "print(\"predicted values : \")\n",
        "print(out, end=\"\\n\")\n",
        "print(\"true values : \")\n",
        "print(y_test[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3X4kXvwn7xz",
        "outputId": "81285721-725a-4d92-e2b6-dad5b873e7d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200, Error = 0.244942\n",
            "Epoch 2/200, Error = 0.099208\n",
            "Epoch 3/200, Error = 0.074093\n",
            "Epoch 4/200, Error = 0.058111\n",
            "Epoch 5/200, Error = 0.046121\n",
            "Epoch 6/200, Error = 0.037907\n",
            "Epoch 7/200, Error = 0.032112\n",
            "Epoch 8/200, Error = 0.027669\n",
            "Epoch 9/200, Error = 0.024160\n",
            "Epoch 10/200, Error = 0.021077\n",
            "Epoch 11/200, Error = 0.018869\n",
            "Epoch 12/200, Error = 0.016970\n",
            "Epoch 13/200, Error = 0.015425\n",
            "Epoch 14/200, Error = 0.014038\n",
            "Epoch 15/200, Error = 0.012750\n",
            "Epoch 16/200, Error = 0.011604\n",
            "Epoch 17/200, Error = 0.010567\n",
            "Epoch 18/200, Error = 0.009794\n",
            "Epoch 19/200, Error = 0.009088\n",
            "Epoch 20/200, Error = 0.008512\n",
            "Epoch 21/200, Error = 0.008015\n",
            "Epoch 22/200, Error = 0.007486\n",
            "Epoch 23/200, Error = 0.007097\n",
            "Epoch 24/200, Error = 0.006901\n",
            "Epoch 25/200, Error = 0.006678\n",
            "Epoch 26/200, Error = 0.006544\n",
            "Epoch 27/200, Error = 0.006083\n",
            "Epoch 28/200, Error = 0.005871\n",
            "Epoch 29/200, Error = 0.005615\n",
            "Epoch 30/200, Error = 0.005279\n",
            "Epoch 31/200, Error = 0.005065\n",
            "Epoch 32/200, Error = 0.004884\n",
            "Epoch 33/200, Error = 0.004683\n",
            "Epoch 34/200, Error = 0.004590\n",
            "Epoch 35/200, Error = 0.004483\n",
            "Epoch 36/200, Error = 0.004463\n",
            "Epoch 37/200, Error = 0.004333\n",
            "Epoch 38/200, Error = 0.004105\n",
            "Epoch 39/200, Error = 0.003981\n",
            "Epoch 40/200, Error = 0.003934\n",
            "Epoch 41/200, Error = 0.003805\n",
            "Epoch 42/200, Error = 0.003766\n",
            "Epoch 43/200, Error = 0.003711\n",
            "Epoch 44/200, Error = 0.003715\n",
            "Epoch 45/200, Error = 0.003604\n",
            "Epoch 46/200, Error = 0.003550\n",
            "Epoch 47/200, Error = 0.003546\n",
            "Epoch 48/200, Error = 0.003534\n",
            "Epoch 49/200, Error = 0.003455\n",
            "Epoch 50/200, Error = 0.003449\n",
            "Epoch 51/200, Error = 0.003295\n",
            "Epoch 52/200, Error = 0.003169\n",
            "Epoch 53/200, Error = 0.003049\n",
            "Epoch 54/200, Error = 0.002986\n",
            "Epoch 55/200, Error = 0.002938\n",
            "Epoch 56/200, Error = 0.002895\n",
            "Epoch 57/200, Error = 0.002854\n",
            "Epoch 58/200, Error = 0.002837\n",
            "Epoch 59/200, Error = 0.002824\n",
            "Epoch 60/200, Error = 0.002821\n",
            "Epoch 61/200, Error = 0.002753\n",
            "Epoch 62/200, Error = 0.002720\n",
            "Epoch 63/200, Error = 0.002707\n",
            "Epoch 64/200, Error = 0.002667\n",
            "Epoch 65/200, Error = 0.002682\n",
            "Epoch 66/200, Error = 0.002705\n",
            "Epoch 67/200, Error = 0.002743\n",
            "Epoch 68/200, Error = 0.002694\n",
            "Epoch 69/200, Error = 0.002547\n",
            "Epoch 70/200, Error = 0.002383\n",
            "Epoch 71/200, Error = 0.002334\n",
            "Epoch 72/200, Error = 0.002333\n",
            "Epoch 73/200, Error = 0.002278\n",
            "Epoch 74/200, Error = 0.002251\n",
            "Epoch 75/200, Error = 0.002185\n",
            "Epoch 76/200, Error = 0.002185\n",
            "Epoch 77/200, Error = 0.002294\n",
            "Epoch 78/200, Error = 0.002208\n",
            "Epoch 79/200, Error = 0.002231\n",
            "Epoch 80/200, Error = 0.002130\n",
            "Epoch 81/200, Error = 0.002154\n",
            "Epoch 82/200, Error = 0.002097\n",
            "Epoch 83/200, Error = 0.002017\n",
            "Epoch 84/200, Error = 0.001979\n",
            "Epoch 85/200, Error = 0.001967\n",
            "Epoch 86/200, Error = 0.001942\n",
            "Epoch 87/200, Error = 0.001953\n",
            "Epoch 88/200, Error = 0.001906\n",
            "Epoch 89/200, Error = 0.001896\n",
            "Epoch 90/200, Error = 0.001877\n",
            "Epoch 91/200, Error = 0.001865\n",
            "Epoch 92/200, Error = 0.001859\n",
            "Epoch 93/200, Error = 0.001845\n",
            "Epoch 94/200, Error = 0.001842\n",
            "Epoch 95/200, Error = 0.001842\n",
            "Epoch 96/200, Error = 0.001860\n",
            "Epoch 97/200, Error = 0.001807\n",
            "Epoch 98/200, Error = 0.001804\n",
            "Epoch 99/200, Error = 0.001787\n",
            "Epoch 100/200, Error = 0.001805\n",
            "Epoch 101/200, Error = 0.001755\n",
            "Epoch 102/200, Error = 0.001743\n",
            "Epoch 103/200, Error = 0.001718\n",
            "Epoch 104/200, Error = 0.001706\n",
            "Epoch 105/200, Error = 0.001687\n",
            "Epoch 106/200, Error = 0.001685\n",
            "Epoch 107/200, Error = 0.001673\n",
            "Epoch 108/200, Error = 0.001687\n",
            "Epoch 109/200, Error = 0.001666\n",
            "Epoch 110/200, Error = 0.001690\n",
            "Epoch 111/200, Error = 0.001624\n",
            "Epoch 112/200, Error = 0.001617\n",
            "Epoch 113/200, Error = 0.001573\n",
            "Epoch 114/200, Error = 0.001591\n",
            "Epoch 115/200, Error = 0.001543\n",
            "Epoch 116/200, Error = 0.001560\n",
            "Epoch 117/200, Error = 0.001536\n",
            "Epoch 118/200, Error = 0.001572\n",
            "Epoch 119/200, Error = 0.001552\n",
            "Epoch 120/200, Error = 0.001574\n",
            "Epoch 121/200, Error = 0.001553\n",
            "Epoch 122/200, Error = 0.001548\n",
            "Epoch 123/200, Error = 0.001515\n",
            "Epoch 124/200, Error = 0.001501\n",
            "Epoch 125/200, Error = 0.001488\n",
            "Epoch 126/200, Error = 0.001480\n",
            "Epoch 127/200, Error = 0.001471\n",
            "Epoch 128/200, Error = 0.001465\n",
            "Epoch 129/200, Error = 0.001457\n",
            "Epoch 130/200, Error = 0.001452\n",
            "Epoch 131/200, Error = 0.001445\n",
            "Epoch 132/200, Error = 0.001438\n",
            "Epoch 133/200, Error = 0.001435\n",
            "Epoch 134/200, Error = 0.001427\n",
            "Epoch 135/200, Error = 0.001435\n",
            "Epoch 136/200, Error = 0.001408\n",
            "Epoch 137/200, Error = 0.001414\n",
            "Epoch 138/200, Error = 0.001394\n",
            "Epoch 139/200, Error = 0.001401\n",
            "Epoch 140/200, Error = 0.001383\n",
            "Epoch 141/200, Error = 0.001392\n",
            "Epoch 142/200, Error = 0.001383\n",
            "Epoch 143/200, Error = 0.001394\n",
            "Epoch 144/200, Error = 0.001389\n",
            "Epoch 145/200, Error = 0.001399\n",
            "Epoch 146/200, Error = 0.001393\n",
            "Epoch 147/200, Error = 0.001372\n",
            "Epoch 148/200, Error = 0.001358\n",
            "Epoch 149/200, Error = 0.001352\n",
            "Epoch 150/200, Error = 0.001345\n",
            "Epoch 151/200, Error = 0.001335\n",
            "Epoch 152/200, Error = 0.001329\n",
            "Epoch 153/200, Error = 0.001322\n",
            "Epoch 154/200, Error = 0.001319\n",
            "Epoch 155/200, Error = 0.001316\n",
            "Epoch 156/200, Error = 0.001338\n",
            "Epoch 157/200, Error = 0.001349\n",
            "Epoch 158/200, Error = 0.001352\n",
            "Epoch 159/200, Error = 0.001335\n",
            "Epoch 160/200, Error = 0.001337\n",
            "Epoch 161/200, Error = 0.001319\n",
            "Epoch 162/200, Error = 0.001319\n",
            "Epoch 163/200, Error = 0.001313\n",
            "Epoch 164/200, Error = 0.001314\n",
            "Epoch 165/200, Error = 0.001312\n",
            "Epoch 166/200, Error = 0.001307\n",
            "Epoch 167/200, Error = 0.001300\n",
            "Epoch 168/200, Error = 0.001294\n",
            "Epoch 169/200, Error = 0.001289\n",
            "Epoch 170/200, Error = 0.001287\n",
            "Epoch 171/200, Error = 0.001285\n",
            "Epoch 172/200, Error = 0.001281\n",
            "Epoch 173/200, Error = 0.001277\n",
            "Epoch 174/200, Error = 0.001274\n",
            "Epoch 175/200, Error = 0.001268\n",
            "Epoch 176/200, Error = 0.001267\n",
            "Epoch 177/200, Error = 0.001259\n",
            "Epoch 178/200, Error = 0.001258\n",
            "Epoch 179/200, Error = 0.001251\n",
            "Epoch 180/200, Error = 0.001250\n",
            "Epoch 181/200, Error = 0.001243\n",
            "Epoch 182/200, Error = 0.001243\n",
            "Epoch 183/200, Error = 0.001238\n",
            "Epoch 184/200, Error = 0.001239\n",
            "Epoch 185/200, Error = 0.001238\n",
            "Epoch 186/200, Error = 0.001241\n",
            "Epoch 187/200, Error = 0.001240\n",
            "Epoch 188/200, Error = 0.001241\n",
            "Epoch 189/200, Error = 0.001236\n",
            "Epoch 190/200, Error = 0.001233\n",
            "Epoch 191/200, Error = 0.001229\n",
            "Epoch 192/200, Error = 0.001229\n",
            "Epoch 193/200, Error = 0.001236\n",
            "Epoch 194/200, Error = 0.001225\n",
            "Epoch 195/200, Error = 0.001223\n",
            "Epoch 196/200, Error = 0.001225\n",
            "Epoch 197/200, Error = 0.001226\n",
            "Epoch 198/200, Error = 0.001245\n",
            "Epoch 199/200, Error = 0.001246\n",
            "Epoch 200/200, Error = 0.001228\n",
            "\n",
            "\n",
            "predicted values : \n",
            "[array([[-5.17671604e-03,  7.90788063e-03, -1.38230997e-02,\n",
            "        -6.44306657e-02,  1.25629796e-02,  2.52683995e-03,\n",
            "        -2.76693690e-04,  9.84272487e-01, -1.29423066e-03,\n",
            "        -6.14833271e-03]]), array([[ 0.03886178, -0.0680157 , -0.07313872,  0.92049646,  0.12314184,\n",
            "        -0.03810243,  0.13398786, -0.09456566, -0.7035011 ,  0.19757406]]), array([[ 1.44388860e-03,  9.90326794e-01,  9.09023121e-05,\n",
            "        -5.45129669e-02,  4.35797968e-02,  3.36550207e-02,\n",
            "         1.94851487e-02,  4.20747744e-03, -4.79383797e-02,\n",
            "        -2.11615899e-02]])]\n",
            "true values : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MyNet.plot_error('Error Rate for MNIST, lr = 0.09')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "LDZLbWEM8o8A",
        "outputId": "eea1810a-3753-4f5d-96dd-0531b83db297"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}